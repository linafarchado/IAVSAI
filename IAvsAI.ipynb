{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAvsAI with old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install nltk\n",
    "#! pip install datasets\n",
    "#! pip install tiktoken\n",
    "#! pip install spacy\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "#import eli5\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tiktoken\n",
    "import nltk\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import json\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupérer les données d'un challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "\n",
    "# Lister les challenges\n",
    "#fs.ls(\"gvimont/diffusion/hackathon-minarm-2024\")\n",
    "\n",
    "# Lister les fichiers d'un challenge\n",
    "fs.ls(\"civel/diffusion/hackathon-minarm-2024/AIVSAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharger les données dans le service\n",
    "PATH_IN = 'civel/diffusion/hackathon-minarm-2024/AIVSAI/HC3.zip'\n",
    "fs.download(PATH_IN, 'data/HC3.zip')\n",
    "\n",
    "# Décompresser les données\n",
    "with zipfile.ZipFile(\"data/HC3.zip\",\"r\") as zip_file:\n",
    "    zip_file.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : les données peuvent être également téléchargées directement si besoin, pour être utilisées hors du SSP CLoud.\n",
    "Exemple pour le fichier ci-dessus (même format de lien pour les autres challenges) : \n",
    "\n",
    "http://minio.lab.sspcloud.fr/gvimont/diffusion/hackathon-minarm-2024/AIVSAI/HC3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporter des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_files():\n",
    "    file_path = \"data/HC3/all.jsonl\"\n",
    "    dfs = pd.read_json(file_path, lines=True)\n",
    "    return dfs\n",
    "\n",
    "df = load_jsonl_files()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question(question):\n",
    "    # Remove if question starts with 'Q.'\n",
    "    return question[3:].strip() if question[:3] == 'Q. ' else question\n",
    "\n",
    "def clean_questions(df):\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    df_cleaned['question'] = df_cleaned['question'].apply(lambda x: clean_question(x))\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_answer(answer):\n",
    "    # Transform list answer into string\n",
    "    return ' '.join(answer) if isinstance(answer, list) else answer\n",
    "\n",
    "def clean_answers(df):\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    df_cleaned['human_answers'] = df_cleaned['human_answers'].apply(lambda x: clean_answer(x))\n",
    "    df_cleaned['chatgpt_answers'] = df_cleaned['chatgpt_answers'].apply(lambda x: clean_answer(x))\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_type_data(df) :\n",
    "    \n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned['human_answers'].astype(str)\n",
    "    df_cleaned['chatgpt_answers'].astype(str)\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    df = clean_answers(df)\n",
    "    df = df.drop_duplicates()\n",
    "    df = clean_questions(df)\n",
    "    df = clean_type_data(df)\n",
    "    if 'index' in df.columns :\n",
    "        df = df.drop(columns=\"index\")\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataframe(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export vers un bucket personnel\n",
    "PATH_OUT = 'misterfacile/diffusion/projet-mongroupe-hackathon/all_dataset.csv'\n",
    "with fs.open(PATH_OUT, 'w') as file_out:\n",
    "    df.to_csv(file_out, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB : le dossier 'diffusion' permet un accès en lecture à tous les membres du groupe !\n",
    "# Tous les membres peuvent donc le voir et l'utiliser dans un service\n",
    "fs.ls(\"misterfacile/diffusion/projet-mongroupe-hackathon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(PATH_OUT, mode=\"r\") as file_in:\n",
    "    df_test = pd.read_csv(file_in)\n",
    "    df_test = clean_dataframe(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['finance','wiki_csai','open_qa','medicine','reddit_eli5']\n",
    "answers_by_source = {source: \" \".join(df.loc[df['source'] == source, 'human_answers']) for source in df['source'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(doc: str) -> list[str]:\n",
    "    return word_tokenize(doc)\n",
    "\n",
    "def gpt_tokenize(doc: str) -> list:\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    tokens = enc.encode(doc)\n",
    "    return [str(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc: str, base_tokenizer=word_tokenize, do_lower=False, do_remove_stop_word=False, custom_stop_words=[], do_lemmatize=False) -> tuple[list,list]:\n",
    "    if do_lower:\n",
    "        doc = doc.lower()\n",
    "    list_token = base_tokenizer(doc)\n",
    "\n",
    "    if do_remove_stop_word:\n",
    "        stop_words = en_stop | set(\"-.!?()_;:,'[]$%*/|\\\\\") | {'...'} | {\"''\"} | {\"``\"} | set(custom_stop_words) \n",
    "        list_token = [token for token in list_token if token not in stop_words]\n",
    "        \n",
    "    if do_lemmatize:\n",
    "        wnl = WordNetLemmatizer()\n",
    "        list_token = [wnl.lemmatize(t) for t in list_token]\n",
    "\n",
    "    return list_token\n",
    "\n",
    "def use_tokenizer(tokenizer, base_tokenizer=word_tokenize):\n",
    "    return tokenizer\n",
    "\n",
    "def remove_stop_words(custom_stop_words):\n",
    "    return custom_stop_words\n",
    "\n",
    "def add_stop_words(words, custom_stop_words):\n",
    "    custom_stop_words.extend(words)\n",
    "    return custom_stop_words\n",
    "\n",
    "def lower():\n",
    "    return True\n",
    "\n",
    "def lemmatize():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "custom_stop_words = []\n",
    "list_token_per_source = {}\n",
    "for source in sources:\n",
    "    list_token_per_source[source] = tokenize(answers_by_source[source], do_lower=True, do_remove_stop_word=True, custom_stop_words=custom_stop_words)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for source, list_token in list_token_per_source.items():\n",
    "    word_counts = Counter(list_token)\n",
    "    top_10_words = word_counts.most_common(10)\n",
    "    print(source,\":\\n\", top_10_words)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"file_path = 'list_token_per_source.json'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, 'w') as json_file:\n",
    "    # Write the dictionary to the file\n",
    "    json.dump(list_token_per_source, json_file, indent=4)\n",
    "\n",
    "print(\"Dictionary has been written to\", file_path)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the length of the answer between human and ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareLengthAnswer(data, category=\"\") :\n",
    "\n",
    "    #Collect the length of the answer\n",
    "    if (category == \"\") :\n",
    "        lengthHumanAnswer = data['human_answers'].apply(len)\n",
    "        lengthChatGPTAnswer = data['chatgpt_answers'].apply(len)\n",
    "    else :\n",
    "        lengthHumanAnswer = data[data['source'] == category]['human_answers'].apply(len)\n",
    "        lengthChatGPTAnswer = data[data['source'] == category]['chatgpt_answers'].apply(len)\n",
    "    \n",
    "    # Display the graphic\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengthHumanAnswer, bins=100, alpha=0.5, label='Human Answers')\n",
    "    plt.hist(lengthChatGPTAnswer, bins=20, alpha=0.5, label='ChatGPT Answers')\n",
    "    plt.title('Comparaison de la taille des réponses')\n",
    "    plt.xlabel('Quantité de caractères par réponse')\n",
    "    plt.ylabel('Quantité de réponse')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlim(left=0)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareLengthAnswer(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = df_test[df_test['source'] == 'wiki_csai']['human_answers']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentences(text):\n",
    "    # Utiliser regex pour détecter la fin des phrases avec plus de précision\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    # Retourner le nombre de phrases, en s'assurant qu'on ne compte pas les éléments vides\n",
    "    return len([sentence for sentence in sentences if sentence.strip()])\n",
    "\n",
    "# Appliquer la fonction de comptage sur les colonnes des réponses et créer de nouvelles colonnes pour les comptes\n",
    "df['human_sentence_count'] = df['human_answers'].apply(count_sentences)\n",
    "df['chatgpt_sentence_count'] = df['chatgpt_answers'].apply(count_sentences)\n",
    "\n",
    "# Grouper par la colonne 'source' et calculer la somme des phrases pour chaque source\n",
    "grouped = df.groupby('source').agg({\n",
    "    'human_sentence_count': 'sum',\n",
    "    'chatgpt_sentence_count': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = np.arange(len(grouped['source']))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "width = 0.35\n",
    "\n",
    "# Create bars\n",
    "human_bars = ax.bar(positions - width/2, grouped['human_sentence_count'],width, label='Human Sentence Count')\n",
    "chatgpt_bars = ax.bar(positions + width/2, grouped['chatgpt_sentence_count'], width, label='ChatGPT Sentence Count')\n",
    "\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('Sentence Count')\n",
    "ax.set_title('Sentence Count Comparison by Source')\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(grouped['source'])\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between answers and questions length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.copy()\n",
    "\n",
    "df_plot['question_length'] = df['question'].apply(len)\n",
    "df_plot['human_answer_length'] = df['human_answers'].apply(len)\n",
    "df_plot['chatgpt_answer_length'] = df['chatgpt_answers'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Scatter plot for Q and A\n",
    "plt.scatter(df_plot['question_length'], df_plot['human_answer_length'], label='Human Answers', alpha=0.3)\n",
    "plt.scatter(df_plot['question_length'], df_plot['chatgpt_answer_length'], label='ChatGPT Answers', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Question Length')\n",
    "plt.ylabel('Answer Length')\n",
    "plt.title('Correlation between Question and Answer Length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(PATH_OUT, mode=\"r\") as file_in:\n",
    "    df_test = pd.read_csv(file_in)\n",
    "    df_test = clean_dataframe(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate\n",
    "\n",
    "# Separate answers\n",
    "chatgpt_answers = df['chatgpt_answers'].values\n",
    "human_answers = df['human_answers'].values\n",
    "\n",
    "# Tokenization & padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(chatgpt_answers) + list(human_answers))\n",
    "MAX_LEN = 100\n",
    "\n",
    "X_chatgpt = pad_sequences(tokenizer.texts_to_sequences(chatgpt_answers), maxlen=MAX_LEN)\n",
    "X_human = pad_sequences(tokenizer.texts_to_sequences(human_answers), maxlen=MAX_LEN)\n",
    "\n",
    "# Concatenate X_chatgpt and X_human\n",
    "X = np.concatenate((X_chatgpt, X_human), axis=0)\n",
    "y = np.array([0] * len(X_chatgpt) + [1] * len(X_human))\n",
    "\n",
    "# Split data into training, testing, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Model\n",
    "input_text = Input(shape=(MAX_LEN,), name='input_text')\n",
    "embed = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(input_text)\n",
    "conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(embed)\n",
    "pool = GlobalMaxPooling1D()(conv)\n",
    "dense1 = Dense(64, activation='relu')(pool)\n",
    "output = Dense(1, activation='sigmoid')(dense1)\n",
    "model = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Logistic with different use of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting training and test sets\n",
    "\n",
    "0: human answers\n",
    "1: chatgpt answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with only answers\n",
    "dataset_answers = pd.DataFrame({\n",
    "    'answers': df['human_answers'].tolist() + df['chatgpt_answers'].tolist(),\n",
    "    'category': [0] * len(df) + [1] * len(df)\n",
    "})\n",
    "\n",
    "#Data with questions and answers\n",
    "dataset_questions_answers = pd.DataFrame({\n",
    "    'answers': df[['question', 'human_answers']].apply(lambda x: ''.join(x), axis=1).tolist() + df[['question', 'chatgpt_answers']].apply(lambda x: ''.join(x), axis=1).tolist(),\n",
    "    'category': [0] * len(df) + [1] * len(df)\n",
    "})\n",
    "\n",
    "#Separating data for questions and answers then concatenate\n",
    "X_questions = df['question'].tolist() + df['question'].tolist()\n",
    "X_answers = df['human_answers'].tolist() + df['chatgpt_answers'].tolist()\n",
    "y_targets = [0] * len(df) + [1] * len(df)\n",
    "\n",
    "X_questions = np.array(X_questions)\n",
    "X_answers = np.array(X_answers)\n",
    "y = np.array(y_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data with only answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = dataset_answers['answers']  # Features (answers)\n",
    "y_a = dataset_answers['category']  # Target variable (category)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_a_train, X_a_test, y_a_train, y_a_test = train_test_split(X_a, y_a, test_size=0.2, random_state=42)\n",
    "\n",
    "# Applying CountVectorizer()\n",
    "text_transformer = CountVectorizer()\n",
    "\n",
    "X_a_train_vectorized = text_transformer.fit_transform(X_a_train)\n",
    "X_a_test_vectorized = text_transformer.transform(X_a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = LogisticRegression(penalty='l2', solver='liblinear', class_weight='balanced')\n",
    "model_a.fit(X_a_train_vectorized, y_a_train)\n",
    "y_a_pred = model_a.predict(X_a_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy_score(y_a_test, y_a_pred)\n",
    "print(\"Classification Report:\\n\", classification_report(y_a_test, y_a_pred))\n",
    "print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data with questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_qa = dataset_questions_answers['answers']  # Features (questions and answers)\n",
    "y_qa = dataset_questions_answers['category']  # Target variable (category)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_qa_train, X_qa_test, y_qa_train, y_qa_test = train_test_split(X_qa, y_qa, test_size=0.2, random_state=42)\n",
    "\n",
    "text_transformer = CountVectorizer()\n",
    "\n",
    "X_qa_train_vectorized = text_transformer.fit_transform(X_qa_train)\n",
    "X_qa_test_vectorized = text_transformer.transform(X_qa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qa = LogisticRegression(penalty='l2', solver='liblinear', class_weight='balanced')\n",
    "model_qa.fit(X_qa_train_vectorized, y_qa_train)\n",
    "y_qa_pred = model_qa.predict(X_qa_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy_score(y_qa_test, y_qa_pred)\n",
    "print(\"Classification Report:\\n\", classification_report(y_qa_test, y_qa_pred))\n",
    "print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating data for questions and answers then concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split my data into training and testing sets\n",
    "X_questions_train, X_questions_test, X_answers_train, X_answers_test, y_train, y_test = train_test_split(X_questions, X_answers, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Preprocess my text data separately for questions and answers\n",
    "vectorizer_question = CountVectorizer()\n",
    "vectorizer_answer = CountVectorizer()\n",
    "\n",
    "X_questions_train_tfidf = vectorizer_question.fit_transform(X_questions_train)\n",
    "X_questions_test_tfidf = vectorizer_question.transform(X_questions_test)\n",
    "\n",
    "X_answers_train_tfidf = vectorizer_answer.fit_transform(X_answers_train)\n",
    "X_answers_test_tfidf = vectorizer_answer.transform(X_answers_test)\n",
    "\n",
    "# Concatenate the TF-IDF representations of questions and answers\n",
    "X_train = hstack([X_questions_train_tfidf, X_answers_train_tfidf])\n",
    "X_test = hstack([X_questions_test_tfidf, X_answers_test_tfidf])\n",
    "\n",
    "# Train the logistic regression model\n",
    "model_3 = LogisticRegression(penalty='l2', solver='liblinear', class_weight='balanced')\n",
    "model_3.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model_3.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=model_3.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_3.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (CountVectorizer) Naives bayes avec MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'un dataset composé de toutes les phrases avec ces labels\n",
    "dataset = pd.DataFrame({\n",
    "    'answers': df['human_answers'].tolist() + df['chatgpt_answers'].tolist(),\n",
    "    'category': ['human'] * len(df) + ['chatgpt'] * len(df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['answers']\n",
    "Y = dataset['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a split dataset avec un poid bien balancé\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction de modèle avec un tokenizer et ngram_range correspondant aux mots adjacents\n",
    "\n",
    "model = make_pipeline(CountVectorizer(tokenizer=word_tokenize, ngram_range = (3,3)), MultinomialNB(alpha=4.0, fit_prior=False))\n",
    "\n",
    "#Construction d'un grid_search pour Naives Bayes\n",
    "# Define the parameter grid\n",
    "#param_grid = {\n",
    "#    'alpha': [1.0, 2.0, 3.0, 4.0],  # Smoothing parameter\n",
    "#    'fit_prior': [True, False]       # Whether to learn class prior probabilities\n",
    "#}\n",
    "#grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "# Fit the grid search to the data\n",
    "#grid_search.fit(X_train_counts, Y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "#best_params = grid_search.best_params_\n",
    "#print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_test, y_pred, labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation and print the mean accuracy\n",
    "scoring = 'f1_macro'\n",
    "scores = cross_val_score(model, X, Y, cv=5, scoring=scoring, n_jobs = -1)\n",
    "print(f\"Mean {scoring}: {scores.mean()}\")\n",
    "print(f\"Standard deviation {scoring}: {scores.std()}\")\n",
    "\n",
    "scores_1 = cross_val_score(model, X, Y, cv=10, scoring=scoring, n_jobs = -1)\n",
    "print(f\"Mean {scoring}: {scores_1.mean()}\")\n",
    "print(f\"Standard deviation {scoring}: {scores_1.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Regression using a set training by Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = dataset['answers']\n",
    "X_strings = \" \".join(X_list)\n",
    "\n",
    "#Decoupage\n",
    "X_sents = sent_tokenize(X_strings)\n",
    "X_sents = [word_tokenize(s) for s in X_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation du model Word2Vec\n",
    "model = Word2Vec(sentences=X_sents, vector_size=50, window=5, min_count=5, workers=4)\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"word2vec.wordvectors\") # Store the words and their trained embeddings\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Charger les vecteurs des mots\n",
    "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc, wv):\n",
    "    \"\"\"Create document vectors by averaging word vectors.\"\"\"\n",
    "    words = word_tokenize(doc)\n",
    "    word_vectors = np.array([wv[word] for word in words if word in wv])\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(wv.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['answers']\n",
    "Y = dataset['category']\n",
    "X = np.array([document_vector(text, wv) for text in X])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=Y_train)\n",
    "\n",
    "print(f\"Features dimension: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(LogisticRegression(random_state=42, class_weight='balanced', solver='liblinear'))\n",
    "\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'logisticregression__penalty': ['l1', 'l2'],\n",
    "    #'logisticregression__max_iter': [10, 50, 100, 200, 300],\n",
    "    #'logisticregression__tol': [1e-4, 1e-3, 1e-2],\n",
    "    'logisticregression__dual': [False, True],\n",
    "    'logisticregression__fit_intercept': [True, False],\n",
    "    'logisticregression__multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "    #'logisticregression__warm_start': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, Y_test)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(Y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
