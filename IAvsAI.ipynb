{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutoriel : interagir avec le système de stockage S3 du SSP Cloud (MinIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install datasets\n",
    "! pip install tiktoken\n",
    "! pip install spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tiktoken\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupérer les données d'un challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "\n",
    "# Lister les challenges\n",
    "#fs.ls(\"gvimont/diffusion/hackathon-minarm-2024\")\n",
    "\n",
    "# Lister les fichiers d'un challenge\n",
    "fs.ls(\"civel/diffusion/hackathon-minarm-2024/AIVSAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharger les données dans le service\n",
    "PATH_IN = 'civel/diffusion/hackathon-minarm-2024/AIVSAI/HC3.zip'\n",
    "fs.download(PATH_IN, 'data/HC3.zip')\n",
    "\n",
    "# Décompresser les données\n",
    "with zipfile.ZipFile(\"data/HC3.zip\",\"r\") as zip_file:\n",
    "    zip_file.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : les données peuvent être également téléchargées directement si besoin, pour être utilisées hors du SSP CLoud.\n",
    "Exemple pour le fichier ci-dessus (même format de lien pour les autres challenges) : \n",
    "\n",
    "http://minio.lab.sspcloud.fr/gvimont/diffusion/hackathon-minarm-2024/AIVSAI/HC3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporter des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_files():\n",
    "    file_path = \"data/HC3/all.jsonl\"\n",
    "    dfs = pd.read_json(file_path, lines=True)\n",
    "    return dfs\n",
    "\n",
    "df = load_jsonl_files()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question(question):\n",
    "    # Remove if question starts with 'Q.'\n",
    "    return question[3:].strip() if question[:3] == 'Q. ' else question\n",
    "\n",
    "def clean_questions(df):\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    df_cleaned['question'] = df_cleaned['question'].apply(lambda x: clean_question(x))\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_answer(answer):\n",
    "    # Transform list answer into string\n",
    "    return ' '.join(answer) if isinstance(answer, list) else answer\n",
    "\n",
    "def clean_answers(df):\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    df_cleaned['human_answers'] = df_cleaned['human_answers'].apply(lambda x: clean_answer(x))\n",
    "    df_cleaned['chatgpt_answers'] = df_cleaned['chatgpt_answers'].apply(lambda x: clean_answer(x))\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_type_data(df) :\n",
    "    \n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned['human_answers'].astype(str)\n",
    "    df_cleaned['chatgpt_answers'].astype(str)\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    df = clean_answers(df)\n",
    "    df = df.drop_duplicates()\n",
    "    df = clean_questions(df)\n",
    "    df = clean_type_data(df)\n",
    "    if 'index' in df.columns :\n",
    "        df = df.drop(columns=\"index\")\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataframe(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export vers un bucket personnel\n",
    "PATH_OUT = 'juliettejin/diffusion/projet-mongroupe-hackathon/all_dataset.csv'\n",
    "with fs.open(PATH_OUT, 'w') as file_out:\n",
    "    df.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB : le dossier 'diffusion' permet un accès en lecture à tous les membres du groupe !\n",
    "# Tous les membres peuvent donc le voir et l'utiliser dans un service\n",
    "fs.ls(\"juliettejin/diffusion/projet-mongroupe-hackathon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(PATH_OUT, mode=\"r\") as file_in:\n",
    "    df_test = pd.read_csv(file_in)\n",
    "    df_test = clean_dataframe(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['finance','wiki_csai','open_qa','medicine','reddit_eli5']\n",
    "answers_by_source = {source: \" \".join(df.loc[df['source'] == source, 'human_answers']) for source in df['source'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(doc: str) -> list[str]:\n",
    "    return word_tokenize(doc)\n",
    "\n",
    "def gpt_tokenize(doc: str) -> list:\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    tokens = enc.encode(doc)\n",
    "    return [str(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc: str, base_tokenizer=word_tokenize, do_lower=False, do_remove_stop_word=False, custom_stop_words=[], do_lemmatize=False) -> tuple[list,list]:\n",
    "    if do_lower:\n",
    "        doc = doc.lower()\n",
    "    list_token = base_tokenizer(doc)\n",
    "\n",
    "    if do_remove_stop_word:\n",
    "        stop_words = en_stop | set(\"-.!?()_;:,'[]$%*/|\\\\\") | {'...'} | {\"''\"} | {\"``\"} | set(custom_stop_words) \n",
    "        list_token = [token for token in list_token if token not in stop_words]\n",
    "        \n",
    "    if do_lemmatize:\n",
    "        wnl = WordNetLemmatizer()\n",
    "        list_token = [wnl.lemmatize(t) for t in list_token]\n",
    "\n",
    "    return list_token\n",
    "\n",
    "def use_tokenizer(tokenizer, base_tokenizer=word_tokenize):\n",
    "    return tokenizer\n",
    "\n",
    "def remove_stop_words(custom_stop_words):\n",
    "    return custom_stop_words\n",
    "\n",
    "def add_stop_words(words, custom_stop_words):\n",
    "    custom_stop_words.extend(words)\n",
    "    return custom_stop_words\n",
    "\n",
    "def lower():\n",
    "    return True\n",
    "\n",
    "def lemmatize():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = []\n",
    "list_token_per_source = {}\n",
    "for source in sources:\n",
    "    list_token_per_source[source] = tokenize(answers_by_source[source], do_lower=True, do_remove_stop_word=True, custom_stop_words=custom_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, list_token in list_token_per_source.items():\n",
    "    word_counts = Counter(list_token)\n",
    "    top_10_words = word_counts.most_common(10)\n",
    "    print(source,\":\\n\", top_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'list_token_per_source.json'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, 'w') as json_file:\n",
    "    # Write the dictionary to the file\n",
    "    json.dump(list_token_per_source, json_file, indent=4)\n",
    "\n",
    "print(\"Dictionary has been written to\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the length of the answer between human and ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareLengthAnswer(data, category=\"\") :\n",
    "\n",
    "    #Collect the length of the answer\n",
    "    if (category == \"\") :\n",
    "        lengthHumanAnswer = data['human_answers'].apply(len)\n",
    "        lengthChatGPTAnswer = data['chatgpt_answers'].apply(len)\n",
    "    else :\n",
    "        lengthHumanAnswer = data[data['source'] == category]['human_answers'].apply(len)\n",
    "        lengthChatGPTAnswer = data[data['source'] == category]['chatgpt_answers'].apply(len)\n",
    "    \n",
    "    # Display the graphic\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengthHumanAnswer, bins=20, alpha=0.5, label='Human Answers')\n",
    "    plt.hist(lengthChatGPTAnswer, bins=20, alpha=0.5, label='ChatGPT Answers')\n",
    "    plt.title('Comparaison de la taille des réponses')\n",
    "    plt.xlabel('Quantité de caractères par réponse')\n",
    "    plt.ylabel('Quantité de réponse')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareLengthAnswer(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = df_test[df_test['source'] == 'wiki_csai']['human_answers']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def count_sentences(text):\n",
    "    # Utiliser regex pour détecter la fin des phrases avec plus de précision\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    # Retourner le nombre de phrases, en s'assurant qu'on ne compte pas les éléments vides\n",
    "    return len([sentence for sentence in sentences if sentence.strip()])\n",
    "\n",
    "# Appliquer la fonction de comptage sur les colonnes des réponses et créer de nouvelles colonnes pour les comptes\n",
    "df['human_sentence_count'] = df['human_answers'].apply(count_sentences)\n",
    "df['chatgpt_sentence_count'] = df['chatgpt_answers'].apply(count_sentences)\n",
    "\n",
    "# Grouper par la colonne 'source' et calculer la somme des phrases pour chaque source\n",
    "grouped = df.groupby('source').agg({\n",
    "    'human_sentence_count': 'sum',\n",
    "    'chatgpt_sentence_count': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "positions = np.arange(len(grouped['source']))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "width = 0.35\n",
    "\n",
    "# Create bars\n",
    "human_bars = ax.bar(positions - width/2, grouped['human_sentence_count'],width, label='Human Sentence Count')\n",
    "chatgpt_bars = ax.bar(positions + width/2, grouped['chatgpt_sentence_count'], width, label='ChatGPT Sentence Count')\n",
    "\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('Sentence Count')\n",
    "ax.set_title('Sentence Count Comparison by Source')\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(grouped['source'])\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between answers and questions length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df_plot = df.copy()\n",
    "\n",
    "df_plot['question_length'] = df['question'].apply(len)\n",
    "df_plot['human_answer_length'] = df['human_answers'].apply(len)\n",
    "df_plot['chatgpt_answer_length'] = df['chatgpt_answers'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Scatter plot for Q and A\n",
    "plt.scatter(df_plot['question_length'], df_plot['human_answer_length'], label='Human Answers', alpha=0.3)\n",
    "plt.scatter(df_plot['question_length'], df_plot['chatgpt_answer_length'], label='ChatGPT Answers', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Question Length')\n",
    "plt.ylabel('Answer Length')\n",
    "plt.title('Correlation between Question and Answer Length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({\n",
    "    'answers': df['human_answers'].tolist() + df['chatgpt_answers'].tolist(),\n",
    "    'category': [0] * len(df) + [1] * len(df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['answers']  # Features (questions)\n",
    "y = dataset['category']  # Target variable (answers)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer = CountVectorizer()\n",
    "X_train_vectorized = text_transformer.fit_transform(X_train)\n",
    "X_test_vectorized = text_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
