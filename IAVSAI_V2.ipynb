{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAVSAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To start the project:\n",
    "\n",
    "- First of all, you'll need Onyxia in order to create a new service and launch Vscode-python-gpu.\n",
    "- Open the github project on it.\n",
    "- Then, please add your Onyxia user's name in the variable below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = \"juliettejin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now\n",
    "- Run the following commands to install the dependencies\n",
    "- After that, restart the kernel\n",
    "- Then you can comment theses commands to not run them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install datasets\n",
    "! pip install spacy\n",
    "! pip uninstall -y scipy\n",
    "! pip install scipy==1.12\n",
    "! pip install tensorflow\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "\n",
    "# List the challenge's files\n",
    "fs.ls(\"civel/diffusion/hackathon-minarm-2024/AIVSAI\")\n",
    "PATH_IN = 'civel/diffusion/hackathon-minarm-2024/AIVSAI/hack_train.csv'\n",
    "fs.download(PATH_IN, 'data/hack_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    file_path = \"data/hack_train.csv\"\n",
    "    return pd.read_csv(filepath_or_buffer=file_path)\n",
    "df = load_csv()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe_new(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "df = clean_dataframe_new(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataset in Onyxia\n",
    "\n",
    "- After running the next cell, you will be able to observe the dataset on Onyxia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT = user_name + '/diffusion/projet-mongroupe-hackathon/hack_train.csv'\n",
    "with fs.open(PATH_OUT, 'w') as file_out:\n",
    "    df.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read From Onyxia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(PATH_OUT, mode=\"r\") as file_in:\n",
    "    df = pd.read_csv(file_in)\n",
    "    df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_of_source = len(df['src'].drop_duplicates())\n",
    "print(\"Total number of source:\", total_number_of_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='label')\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Equal data distribution for classification is advantageous as it prevents bias, aids generalization, ensures fair evaluation, and enhances learning stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the length of the answers between human and machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareLengthAnswer(data) :\n",
    "\n",
    "    #Collect the length of the answer\n",
    "    lengthHumanAnswer = data[data['label'] ==  1]['text'].apply(len)\n",
    "    lengthChatGPTAnswer = data[data['label'] ==  0]['text'].apply(len)\n",
    "    \n",
    "    # Display the graphic\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengthHumanAnswer, bins=100, alpha=0.5, label='Human Answers')\n",
    "    plt.hist(lengthChatGPTAnswer, bins=20, alpha=0.5, label='Machine Answers')\n",
    "    plt.title(' Comparaison: the length of the answers between human and machine')\n",
    "    plt.xlabel('Number of caracters per response')\n",
    "    plt.ylabel('Number of response')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlim(left=0)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareLengthAnswer(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that human answers are likely to be longer than machine answers\n",
    "- Human answers tend to be longer because they include additional context and elaboration, aiming for comprehensive understanding.\n",
    "- AI-generated responses prioritize brevity while still conveying essential information efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of sentences per source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentences(text):\n",
    "    # Using regex to detect end of sentense more precisely\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return len([sentence for sentence in sentences if sentence.strip()])\n",
    "\n",
    "df_final = df.copy()\n",
    "df_final['human_sentence_count'] = df_final[df_final['label'] ==  1]['text'].apply(count_sentences)\n",
    "df_final['chatgpt_sentence_count'] = df_final[df_final['label'] ==  0]['text'].apply(count_sentences)\n",
    "\n",
    "grouped = df_final.groupby('src').agg({\n",
    "    'human_sentence_count': 'sum',\n",
    "    'chatgpt_sentence_count': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(grouped['src'], grouped['human_sentence_count'], color='skyblue', label='Human Sentence Count')\n",
    "plt.bar(grouped['src'], grouped['chatgpt_sentence_count'], color='salmon', label='Machine Sentence Count')\n",
    "plt.ylabel('Sentence Count')\n",
    "plt.xlabel('Source')\n",
    "plt.title('Comparison of Sentence Counts')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayésien (nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nb = df.copy()\n",
    "dataset_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_nb['text']\n",
    "Y = dataset_nb['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "\n",
    "- In this section we are using a grid search for Naives Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model construction with a tokenizer and ngram_range\n",
    "model = make_pipeline(CountVectorizer(tokenizer=word_tokenize, ngram_range = (2,2)), MultinomialNB(alpha=5.0, fit_prior=False))\n",
    "\n",
    "# Grid search for Naives Bayes\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'multinomialnb__alpha': [1.0, 2.0, 3.0],        # Smoothing parameter\n",
    "    'multinomialnb__fit_prior': [True, False]       # Whether to learn class prior probabilities\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirecting output to a file\n",
    "\n",
    "#gridsearch_nb_file = 'gridsearch_nb.txt'\n",
    "#with open(gridsearch_nb_file, 'w') as f:\n",
    "#    print(\"Best parameters:\", grid_search.best_params_, file=f)\n",
    "#    print(\"Best cross-validation score:\", grid_search.best_score_, file=f)\n",
    "#    best_model = grid_search.best_estimator_\n",
    "#    test_score = best_model.score(X_lr_test, y_lr_test)\n",
    "#    print(\"Test set score:\", test_score, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_naive_bayes = classification_report(Y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report_naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the classification report to the file\n",
    "#file_path = 'classification_report_naive_bayes.txt'\n",
    "#with open(file_path, 'w') as file:\n",
    "#    file.write(classification_report_naive_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_test, y_pred, labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation and print the mean accuracy\n",
    "scoring = 'f1_macro'\n",
    "scores = cross_val_score(model, X, Y, cv=5, scoring=scoring, n_jobs = -1)\n",
    "print(f\"Mean {scoring}: {scores.mean()}\")\n",
    "print(f\"Standard deviation {scoring}: {scores.std()}\")\n",
    "\n",
    "scores_1 = cross_val_score(model, X, Y, cv=10, scoring=scoring, n_jobs = -1)\n",
    "print(f\"Mean {scoring}: {scores_1.mean()}\")\n",
    "print(f\"Standard deviation {scoring}: {scores_1.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate text and labels\n",
    "texts = df['text'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Tokenization & padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "MAX_LEN = 100\n",
    "\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN)\n",
    "\n",
    "# Invert label mapping\n",
    "label_mapping = {0: 1, 1: 0}\n",
    "\n",
    "if all(label in [0, 1] for label in labels):\n",
    "    label_mapping = {0: 1, 1: 0}\n",
    "\n",
    "labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Split data into training, testing, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Model\n",
    "input_text = Input(shape=(MAX_LEN,), name='input_text')\n",
    "embed = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(input_text)\n",
    "conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(embed)\n",
    "pool = GlobalMaxPooling1D()(conv)\n",
    "dense1 = Dense(64, activation='relu')(pool)\n",
    "output = Dense(1, activation='sigmoid')(dense1)\n",
    "cnn_model = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# Compile\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "cnn_model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "loss, accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Prédire les étiquettes sur les données de test\n",
    "y_pred = (cnn_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Générer et imprimer le rapport de classification\n",
    "print(\"Classification Report for CNN Model:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculer la précision, le rappel et le score F1 pour les deux classes\n",
    "precision_class_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall_class_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "f1_class_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "precision_class_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall_class_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1_class_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "print(\"Metrics for class 0:\")\n",
    "print(\"Precision:\", precision_class_0)\n",
    "print(\"Recall:\", recall_class_0)\n",
    "print(\"F1-score:\", f1_class_0)\n",
    "\n",
    "print(\"\\nMetrics for class 1:\")\n",
    "print(\"Precision:\", precision_class_1)\n",
    "print(\"Recall:\", recall_class_1)\n",
    "print(\"F1-score:\", f1_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense, Dropout, Flatten\n",
    "\n",
    "# Tokenization & padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "MAX_LEN = 100\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN)\n",
    "y = labels\n",
    "\n",
    "# Split data into training, testing, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Reshape input data\n",
    "X_train = X_train.reshape(X_train.shape[0], MAX_LEN)\n",
    "X_val = X_val.reshape(X_val.shape[0], MAX_LEN)\n",
    "X_test = X_test.reshape(X_test.shape[0], MAX_LEN)\n",
    "\n",
    "# Transformer Model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = MAX_LEN\n",
    "\n",
    "input_layer = Input(shape=(max_len,), name='transformer_input')\n",
    "embed = Embedding(vocab_size, 128, mask_zero=True)(input_layer)\n",
    "attention = MultiHeadAttention(num_heads=8, key_dim=64)(embed, embed)\n",
    "attention = LayerNormalization()(attention + embed)\n",
    "dense = Dense(64, activation='relu')(attention)\n",
    "dropout = Dropout(0.1)(dense)\n",
    "output = Dense(64, activation='relu')(dropout)\n",
    "output = Flatten()(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "transformer_model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# Compile Transformer model\n",
    "transformer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Transformer model\n",
    "transformer_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Transformer model\n",
    "loss, accuracy = transformer_model.evaluate(X_test, y_test)\n",
    "print(f'Transformer Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Prédire les étiquettes sur les données de test\n",
    "y_pred = (transformer_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Générer et imprimer le rapport de classification\n",
    "print(\"Classification Report for Transformer Model:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculer la précision, le rappel et le score F1 pour les deux classes\n",
    "precision_class_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall_class_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "f1_class_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "precision_class_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall_class_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1_class_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "print(\"Metrics for class 0:\")\n",
    "print(\"Precision:\", precision_class_0)\n",
    "print(\"Recall:\", recall_class_0)\n",
    "print(\"F1-score:\", f1_class_0)\n",
    "\n",
    "print(\"\\nMetrics for class 1:\")\n",
    "print(\"Precision:\", precision_class_1)\n",
    "print(\"Recall:\", recall_class_1)\n",
    "print(\"F1-score:\", f1_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression (lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = df\n",
    "\n",
    "X_lr = df_lr['text']\n",
    "y_lr = df_lr['label']\n",
    "\n",
    "X_lr_train, X_lr_test, y_lr_train, y_lr_test = train_test_split(X_lr, y_lr, test_size=0.2, random_state=42)\n",
    "\n",
    "text_transformer = CountVectorizer()\n",
    "\n",
    "X_lr_train_vectorized = text_transformer.fit_transform(X_lr_train)\n",
    "X_lr_test_vectorized = text_transformer.transform(X_lr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "\n",
    "- In this cell we will create out model and train it, which will take some time.\n",
    "- You can skip this, and directly load the model that we stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced')\n",
    "model_lr.fit(X_lr_train_vectorized, y_lr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "#dump(model_lr, 'trained_model_lr.joblib')\n",
    "model_lr = load('trained_model_lr.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lr_pred = model_lr.predict(X_lr_test_vectorized)\n",
    "test_accuracy_lr = accuracy_score(y_lr_test, y_lr_pred)\n",
    "classification_report_lr = classification_report(y_lr_test, y_lr_pred)\n",
    "print(\"Classification Report:\\n\", classification_report_lr)\n",
    "print(\"Testing Accuracy:\", test_accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the classification report to the file\n",
    "#file_path = 'classification_report_logistic_regression.txt'\n",
    "#with open(file_path, 'w') as file:\n",
    "#    file.write(classification_report_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_lr_test, y_lr_pred, labels=model_lr.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_lr.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix evaluates our model's performance:\n",
    "\n",
    "- True Negative (TN): 4940\n",
    "- False Positive (FP): 612\n",
    "- False Negative (FN): 823\n",
    "- True Positive (TP): 4989\n",
    "\n",
    "And there is still room for improvement. We can now use a grid search to get the best parameters for our model.\n",
    "You can skip this cell, and print the results in the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(CountVectorizer(), LogisticRegression(random_state=42, class_weight='balanced', solver='liblinear'))\n",
    "\n",
    "param_grid = {\n",
    "    'countvectorizer__lowercase': [False, True],\n",
    "    'countvectorizer__tokenizer': [word_tokenize],\n",
    "    'countvectorizer__ngram_range': [(1, 1), (2, 2)],\n",
    "    'countvectorizer__stop_words': [None, 'english'],\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1],\n",
    "    'logisticregression__penalty': ['l1', 'l2'],\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_lr_train, y_lr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_lr_file = 'gridsearch_lr.txt'\n",
    "\n",
    "# Redirecting output to a file\n",
    "\n",
    "#with open('gridsearch_lr.txt', 'w') as f:\n",
    "#    print(\"Best parameters:\", grid_search.best_params_, file=f)\n",
    "#    print(\"Best cross-validation score:\", grid_search.best_score_, file=f)\n",
    "#    best_model = grid_search.best_estimator_\n",
    "#    test_score = best_model.score(X_lr_test, y_lr_test)\n",
    "#    print(\"Test set score:\", test_score, file=f)\n",
    "\n",
    "with open(gridsearch_lr_file, 'r') as file:\n",
    "    gridsearch_contents = file.read()\n",
    "print(gridsearch_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
