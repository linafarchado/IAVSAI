{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tiktoken\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "\n",
    "# Lister les challenges\n",
    "#fs.ls(\"gvimont/diffusion/hackathon-minarm-2024\")\n",
    "\n",
    "# Lister les fichiers d'un challenge\n",
    "fs.ls(\"civel/diffusion/hackathon-minarm-2024/AIVSAI\")\n",
    "PATH_IN = 'civel/diffusion/hackathon-minarm-2024/AIVSAI/hack_train.csv'\n",
    "fs.download(PATH_IN, 'data/hack_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    file_path = \"data/hack_train.csv\"\n",
    "    return pd.read_csv(filepath_or_buffer=file_path)\n",
    "df = load_csv()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe_new(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "df = clean_dataframe_new(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataset in Onyxia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT = 'linafarchado/diffusion/projet-mongroupe-hackathon/hack_train.csv'\n",
    "with fs.open(PATH_OUT, 'w') as file_out:\n",
    "    df.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read From Onyxia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(PATH_OUT, mode=\"r\") as file_in:\n",
    "    df = pd.read_csv(file_in)\n",
    "    df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate\n",
    "\n",
    "# Separate text and labels\n",
    "texts = df['text'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Tokenization & padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "MAX_LEN = 100\n",
    "\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN)\n",
    "\n",
    "# Invert label mapping\n",
    "label_mapping = {0: 1, 1: 0}\n",
    "\n",
    "if all(label in [0, 1] for label in labels):\n",
    "    label_mapping = {0: 1, 1: 0}\n",
    "\n",
    "labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Split data into training, testing, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Model\n",
    "input_text = Input(shape=(MAX_LEN,), name='input_text')\n",
    "embed = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(input_text)\n",
    "conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(embed)\n",
    "pool = GlobalMaxPooling1D()(conv)\n",
    "dense1 = Dense(64, activation='relu')(pool)\n",
    "output = Dense(1, activation='sigmoid')(dense1)\n",
    "cnn_model = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# Compile\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "cnn_model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "loss, accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Prédire les étiquettes sur les données de test\n",
    "y_pred = (cnn_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Générer et imprimer le rapport de classification\n",
    "print(\"Classification Report for CNN Model:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculer la précision, le rappel et le score F1 pour les deux classes\n",
    "precision_class_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall_class_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "f1_class_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "precision_class_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall_class_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1_class_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "print(\"Metrics for class 0:\")\n",
    "print(\"Precision:\", precision_class_0)\n",
    "print(\"Recall:\", recall_class_0)\n",
    "print(\"F1-score:\", f1_class_0)\n",
    "\n",
    "print(\"\\nMetrics for class 1:\")\n",
    "print(\"Precision:\", precision_class_1)\n",
    "print(\"Recall:\", recall_class_1)\n",
    "print(\"F1-score:\", f1_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense, Dropout, Flatten\n",
    "\n",
    "# Tokenization & padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "MAX_LEN = 100\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN)\n",
    "y = labels\n",
    "\n",
    "# Split data into training, testing, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Reshape input data\n",
    "X_train = X_train.reshape(X_train.shape[0], MAX_LEN)\n",
    "X_val = X_val.reshape(X_val.shape[0], MAX_LEN)\n",
    "X_test = X_test.reshape(X_test.shape[0], MAX_LEN)\n",
    "\n",
    "# Transformer Model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = MAX_LEN\n",
    "\n",
    "input_layer = Input(shape=(max_len,), name='transformer_input')\n",
    "embed = Embedding(vocab_size, 128, mask_zero=True)(input_layer)\n",
    "attention = MultiHeadAttention(num_heads=8, key_dim=64)(embed, embed)\n",
    "attention = LayerNormalization()(attention + embed)\n",
    "dense = Dense(64, activation='relu')(attention)\n",
    "dropout = Dropout(0.1)(dense)\n",
    "output = Dense(64, activation='relu')(dropout)\n",
    "output = Flatten()(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "transformer_model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# Compile Transformer model\n",
    "transformer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Transformer model\n",
    "transformer_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Transformer model\n",
    "loss, accuracy = transformer_model.evaluate(X_test, y_test)\n",
    "print(f'Transformer Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Prédire les étiquettes sur les données de test\n",
    "y_pred = (transformer_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Générer et imprimer le rapport de classification\n",
    "print(\"Classification Report for Transformer Model:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculer la précision, le rappel et le score F1 pour les deux classes\n",
    "precision_class_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall_class_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "f1_class_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "precision_class_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall_class_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1_class_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "print(\"Metrics for class 0:\")\n",
    "print(\"Precision:\", precision_class_0)\n",
    "print(\"Recall:\", recall_class_0)\n",
    "print(\"F1-score:\", f1_class_0)\n",
    "\n",
    "print(\"\\nMetrics for class 1:\")\n",
    "print(\"Precision:\", precision_class_1)\n",
    "print(\"Recall:\", recall_class_1)\n",
    "print(\"F1-score:\", f1_class_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
