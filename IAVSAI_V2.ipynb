{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO READ -- RUN THESES COMMANDS TO INSTALL THE DEPENDENCIES AFTER THAT RESTART THE KERNEL THEN COMMENT THESES COMMAND TO NOT RUN AGAIN\n",
    "#! pip install nltk\n",
    "#! pip install datasets\n",
    "#! pip install tiktoken\n",
    "#! pip install spacy\n",
    "#! pip uninstall -y scipy\n",
    "#! pip install scipy==1.12\n",
    "#! pip install tensorflow\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tiktoken\n",
    "import nltk\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "\n",
    "# Lister les challenges\n",
    "#fs.ls(\"gvimont/diffusion/hackathon-minarm-2024\")\n",
    "\n",
    "# Lister les fichiers d'un challenge\n",
    "fs.ls(\"civel/diffusion/hackathon-minarm-2024/AIVSAI\")\n",
    "PATH_IN = 'civel/diffusion/hackathon-minarm-2024/AIVSAI/hack_train.csv'\n",
    "fs.download(PATH_IN, 'data/hack_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    file_path = \"data/hack_train.csv\"\n",
    "    return pd.read_csv(filepath_or_buffer=file_path)\n",
    "df = load_csv()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe_new(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "df = clean_dataframe_new(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataset in Onyxia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT = 'linafarchado/diffusion/projet-mongroupe-hackathon/hack_train.csv'\n",
    "with fs.open(PATH_OUT, 'w') as file_out:\n",
    "    df.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read From Onyxia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(PATH_OUT, mode=\"r\") as file_in:\n",
    "    df = pd.read_csv(file_in)\n",
    "    df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the length of the answers between human and machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareLengthAnswer(data) :\n",
    "\n",
    "    #Collect the length of the answer\n",
    "    lengthHumanAnswer = data[data['label'] ==  1]['text'].apply(len)\n",
    "    lengthChatGPTAnswer = data[data['label'] ==  0]['text'].apply(len)\n",
    "    \n",
    "    # Display the graphic\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengthHumanAnswer, bins=100, alpha=0.5, label='Human Answers')\n",
    "    plt.hist(lengthChatGPTAnswer, bins=20, alpha=0.5, label='Machine Answers')\n",
    "    plt.title('Comparaison de la taille des réponses')\n",
    "    plt.xlabel('Quantité de caractères par réponse')\n",
    "    plt.ylabel('Quantité de réponse')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlim(left=0)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareLengthAnswer(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentences(text):\n",
    "    # Utiliser regex pour détecter la fin des phrases avec plus de précision\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    # Retourner le nombre de phrases, en s'assurant qu'on ne compte pas les éléments vides\n",
    "    return len([sentence for sentence in sentences if sentence.strip()])\n",
    "\n",
    "df_final = df.copy()\n",
    "# Appliquer la fonction de comptage sur les colonnes des réponses et créer de nouvelles colonnes pour les comptes\n",
    "df_final['human_sentence_count'] = df_final[df_final['label'] ==  1]['text'].apply(count_sentences)\n",
    "df_final['chatgpt_sentence_count'] = df_final[df_final['label'] ==  0]['text'].apply(count_sentences)\n",
    "\n",
    "# Grouper par la colonne 'source' et calculer la somme des phrases pour chaque source\n",
    "grouped = df_final.groupby('src').agg({\n",
    "    'human_sentence_count': 'sum',\n",
    "    'chatgpt_sentence_count': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = np.arange(len(grouped['src']))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "width = 0.35\n",
    "\n",
    "# Create bars\n",
    "human_bars = ax.bar(positions - width/2, grouped['human_sentence_count'],width, label='Human Sentence Count')\n",
    "chatgpt_bars = ax.bar(positions + width/2, grouped['chatgpt_sentence_count'], width, label='ChatGPT Sentence Count')\n",
    "\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('Sentence Count')\n",
    "ax.set_title('Sentence Count Comparison by Source')\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(grouped['src'])\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayésien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'un dataset composé de toutes les phrases avec ces labels\n",
    "dataset = df.copy()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['text']\n",
    "Y = dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a split dataset avec un poid bien balancé\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction de modèle avec un tokenizer et ngram_range correspondant aux mots adjacents\n",
    "\n",
    "model = make_pipeline(CountVectorizer(tokenizer=word_tokenize, ngram_range = (3,3)), MultinomialNB(alpha=5.0, fit_prior=False))\n",
    "\n",
    "#Construction d'un GRID SEARCH pour Naives Bayes\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "#param_grid = {\n",
    "#    'multinomialnb__alpha': [1.0, 2.0, 3.0, 4.0, 5.0],  # Smoothing parameter\n",
    "#    'multinomialnb__fit_prior': [True, False]       # Whether to learn class prior probabilities\n",
    "#}\n",
    "#grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "# Fit the grid search to the data\n",
    "#grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "#best_params = grid_search.best_params_\n",
    "#print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_test, y_pred, labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation and print the mean accuracy\n",
    "scoring = 'f1_macro'\n",
    "scores = cross_val_score(model, X, Y, cv=5, scoring=scoring, n_jobs = -1)\n",
    "print(f\"Mean {scoring}: {scores.mean()}\")\n",
    "print(f\"Standard deviation {scoring}: {scores.std()}\")\n",
    "\n",
    "scores_1 = cross_val_score(model, X, Y, cv=10, scoring=scoring, n_jobs = -1)\n",
    "print(f\"Mean {scoring}: {scores_1.mean()}\")\n",
    "print(f\"Standard deviation {scoring}: {scores_1.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate\n",
    "\n",
    "# Separate text and labels\n",
    "texts = df['text'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Tokenization & padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "MAX_LEN = 100\n",
    "\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN)\n",
    "\n",
    "# Invert label mapping\n",
    "label_mapping = {0: 1, 1: 0}\n",
    "\n",
    "if all(label in [0, 1] for label in labels):\n",
    "    label_mapping = {0: 1, 1: 0}\n",
    "\n",
    "labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Split data into training, testing, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Model\n",
    "input_text = Input(shape=(MAX_LEN,), name='input_text')\n",
    "embed = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(input_text)\n",
    "conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(embed)\n",
    "pool = GlobalMaxPooling1D()(conv)\n",
    "dense1 = Dense(64, activation='relu')(pool)\n",
    "output = Dense(1, activation='sigmoid')(dense1)\n",
    "cnn_model = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# Compile\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "cnn_model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "loss, accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Prédire les étiquettes sur les données de test\n",
    "y_pred = (cnn_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Générer et imprimer le rapport de classification\n",
    "print(\"Classification Report for CNN Model:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculer la précision, le rappel et le score F1 pour les deux classes\n",
    "precision_class_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall_class_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "f1_class_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "precision_class_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall_class_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1_class_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "print(\"Metrics for class 0:\")\n",
    "print(\"Precision:\", precision_class_0)\n",
    "print(\"Recall:\", recall_class_0)\n",
    "print(\"F1-score:\", f1_class_0)\n",
    "\n",
    "print(\"\\nMetrics for class 1:\")\n",
    "print(\"Precision:\", precision_class_1)\n",
    "print(\"Recall:\", recall_class_1)\n",
    "print(\"F1-score:\", f1_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense, Dropout, Flatten\n",
    "\n",
    "# Tokenization & padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "MAX_LEN = 100\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN)\n",
    "y = labels\n",
    "\n",
    "# Split data into training, testing, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Reshape input data\n",
    "X_train = X_train.reshape(X_train.shape[0], MAX_LEN)\n",
    "X_val = X_val.reshape(X_val.shape[0], MAX_LEN)\n",
    "X_test = X_test.reshape(X_test.shape[0], MAX_LEN)\n",
    "\n",
    "# Transformer Model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = MAX_LEN\n",
    "\n",
    "input_layer = Input(shape=(max_len,), name='transformer_input')\n",
    "embed = Embedding(vocab_size, 128, mask_zero=True)(input_layer)\n",
    "attention = MultiHeadAttention(num_heads=8, key_dim=64)(embed, embed)\n",
    "attention = LayerNormalization()(attention + embed)\n",
    "dense = Dense(64, activation='relu')(attention)\n",
    "dropout = Dropout(0.1)(dense)\n",
    "output = Dense(64, activation='relu')(dropout)\n",
    "output = Flatten()(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "transformer_model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# Compile Transformer model\n",
    "transformer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Transformer model\n",
    "transformer_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Transformer model\n",
    "loss, accuracy = transformer_model.evaluate(X_test, y_test)\n",
    "print(f'Transformer Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Prédire les étiquettes sur les données de test\n",
    "y_pred = (transformer_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Générer et imprimer le rapport de classification\n",
    "print(\"Classification Report for Transformer Model:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculer la précision, le rappel et le score F1 pour les deux classes\n",
    "precision_class_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall_class_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "f1_class_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "precision_class_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall_class_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1_class_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "print(\"Metrics for class 0:\")\n",
    "print(\"Precision:\", precision_class_0)\n",
    "print(\"Recall:\", recall_class_0)\n",
    "print(\"F1-score:\", f1_class_0)\n",
    "\n",
    "print(\"\\nMetrics for class 1:\")\n",
    "print(\"Precision:\", precision_class_1)\n",
    "print(\"Recall:\", recall_class_1)\n",
    "print(\"F1-score:\", f1_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression (lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = df\n",
    "\n",
    "X_lr = df_lr['text']\n",
    "y_lr = df_lr['label']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_lr_train, X_lr_test, y_lr_train, y_lr_test = train_test_split(X_lr, y_lr, test_size=0.2, random_state=42)\n",
    "\n",
    "# Applying CountVectorizer()\n",
    "text_transformer = CountVectorizer()\n",
    "\n",
    "X_lr_train_vectorized = text_transformer.fit_transform(X_lr_train)\n",
    "X_lr_test_vectorized = text_transformer.transform(X_lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced')\n",
    "model_lr.fit(X_lr_train_vectorized, y_lr_train)\n",
    "y_lr_pred = model_lr.predict(X_lr_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_lr = accuracy_score(y_lr_test, y_lr_pred)\n",
    "print(\"Classification Report:\\n\", classification_report(y_lr_test, y_lr_pred))\n",
    "print(\"Testing Accuracy:\", test_accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_lr_test, y_lr_pred, labels=model_lr.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_lr.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = make_pipeline(CountVectorizer(), LogisticRegression(random_state=42, class_weight='balanced', solver='liblinear'))\n",
    "\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'logisticregression__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model_lr, param_grid, cv=5, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_lr_train, y_lr_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_lr_test, y_lr_test)\n",
    "print(\"Test set score:\", test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
