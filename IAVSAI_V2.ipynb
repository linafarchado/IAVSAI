{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tiktoken\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "\n",
    "# Lister les challenges\n",
    "#fs.ls(\"gvimont/diffusion/hackathon-minarm-2024\")\n",
    "\n",
    "# Lister les fichiers d'un challenge\n",
    "fs.ls(\"civel/diffusion/hackathon-minarm-2024/AIVSAI\")\n",
    "PATH_IN = 'civel/diffusion/hackathon-minarm-2024/AIVSAI/hack_train.csv'\n",
    "fs.download(PATH_IN, 'data/hack_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Little disclaimer: this deals with US laws and...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Read: Mentally Retarded Downs. See, we've got ...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If any of you frequent rbadhistory, there is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I believe in a flat tax system, where everyone...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edit: Ok guy's, my views have been changed on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56814</th>\n",
       "      <td>We consider the recovery of a source term f (x...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56815</th>\n",
       "      <td>Self-supervised learning (SlfSL), aiming at le...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56816</th>\n",
       "      <td>Recurrent neural networks (RNNs) have achieved...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56817</th>\n",
       "      <td>Deep reinforcement learning (DRL) is a booming...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56818</th>\n",
       "      <td>As part of Smart Cities initiatives, national,...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56819 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label            src\n",
       "0      Little disclaimer: this deals with US laws and...      1      cmv_human\n",
       "1      Read: Mentally Retarded Downs. See, we've got ...      1      cmv_human\n",
       "2      If any of you frequent rbadhistory, there is a...      1      cmv_human\n",
       "3      I believe in a flat tax system, where everyone...      1      cmv_human\n",
       "4      Edit: Ok guy's, my views have been changed on ...      1      cmv_human\n",
       "...                                                  ...    ...            ...\n",
       "56814  We consider the recovery of a source term f (x...      1  sci_gen_human\n",
       "56815  Self-supervised learning (SlfSL), aiming at le...      1  sci_gen_human\n",
       "56816  Recurrent neural networks (RNNs) have achieved...      1  sci_gen_human\n",
       "56817  Deep reinforcement learning (DRL) is a booming...      1  sci_gen_human\n",
       "56818  As part of Smart Cities initiatives, national,...      1  sci_gen_human\n",
       "\n",
       "[56819 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_csv():\n",
    "    file_path = \"data/hack_train.csv\"\n",
    "    return pd.read_csv(filepath_or_buffer=file_path)\n",
    "df = load_csv()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Little disclaimer: this deals with US laws and...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Read: Mentally Retarded Downs. See, we've got ...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If any of you frequent rbadhistory, there is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I believe in a flat tax system, where everyone...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edit: Ok guy's, my views have been changed on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56814</th>\n",
       "      <td>We consider the recovery of a source term f (x...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56815</th>\n",
       "      <td>Self-supervised learning (SlfSL), aiming at le...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56816</th>\n",
       "      <td>Recurrent neural networks (RNNs) have achieved...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56817</th>\n",
       "      <td>Deep reinforcement learning (DRL) is a booming...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56818</th>\n",
       "      <td>As part of Smart Cities initiatives, national,...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56819 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label            src\n",
       "0      Little disclaimer: this deals with US laws and...      1      cmv_human\n",
       "1      Read: Mentally Retarded Downs. See, we've got ...      1      cmv_human\n",
       "2      If any of you frequent rbadhistory, there is a...      1      cmv_human\n",
       "3      I believe in a flat tax system, where everyone...      1      cmv_human\n",
       "4      Edit: Ok guy's, my views have been changed on ...      1      cmv_human\n",
       "...                                                  ...    ...            ...\n",
       "56814  We consider the recovery of a source term f (x...      1  sci_gen_human\n",
       "56815  Self-supervised learning (SlfSL), aiming at le...      1  sci_gen_human\n",
       "56816  Recurrent neural networks (RNNs) have achieved...      1  sci_gen_human\n",
       "56817  Deep reinforcement learning (DRL) is a booming...      1  sci_gen_human\n",
       "56818  As part of Smart Cities initiatives, national,...      1  sci_gen_human\n",
       "\n",
       "[56819 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_dataframe_new(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "df = clean_dataframe_new(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataset in Onyxia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT = 'linafarchado/diffusion/projet-mongroupe-hackathon/hack_train.csv'\n",
    "with fs.open(PATH_OUT, 'w') as file_out:\n",
    "    df.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read From Onyxia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(PATH_OUT, mode=\"r\") as file_in:\n",
    "    df = pd.read_csv(file_in)\n",
    "    df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Little disclaimer: this deals with US laws and...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Read: Mentally Retarded Downs. See, we've got ...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If any of you frequent rbadhistory, there is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I believe in a flat tax system, where everyone...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edit: Ok guy's, my views have been changed on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label        src\n",
       "0  Little disclaimer: this deals with US laws and...      1  cmv_human\n",
       "1  Read: Mentally Retarded Downs. See, we've got ...      1  cmv_human\n",
       "2  If any of you frequent rbadhistory, there is a...      1  cmv_human\n",
       "3  I believe in a flat tax system, where everyone...      1  cmv_human\n",
       "4  Edit: Ok guy's, my views have been changed on ...      1  cmv_human"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/mamba/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (3.3.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/mamba/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/mamba/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/mamba/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/mamba/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/mamba/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/mamba/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/mamba/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/mamba/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/mamba/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/mamba/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/mamba/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/mamba/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 10:00:31.575541: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 10:00:32.550676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate\n",
    "\n",
    "def CNNModel(df):\n",
    "    # Separate text and labels\n",
    "    texts = df['text'].values\n",
    "    labels = df['label'].values\n",
    "\n",
    "    # Tokenization & padding\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    MAX_LEN = 100\n",
    "\n",
    "    X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN)\n",
    "\n",
    "    # Invert label mapping\n",
    "    label_mapping = {0: 1, 1: 0}\n",
    "\n",
    "    if all(label in [0, 1] for label in labels):\n",
    "        label_mapping = {0: 1, 1: 0}\n",
    "\n",
    "    labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "    # Split data into training, testing, and validation sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Model\n",
    "    input_text = Input(shape=(MAX_LEN,), name='input_text')\n",
    "    embed = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(input_text)\n",
    "    conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(embed)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "    dense1 = Dense(64, activation='relu')(pool)\n",
    "    output = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Evaluation\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 10:00:46.796328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 696 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:86:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713952848.576930  199096 service.cc:145] XLA service 0x7f3d6c009600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1713952848.576978  199096 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-04-24 10:00:48.626234: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-24 10:00:48.857674: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8900\n",
      "2024-04-24 10:00:49.359540: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 10:00:49.519641: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.03GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  28/1066\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.5142 - loss: 0.6929"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1713952850.191661  199096 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1060/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6509 - loss: 0.6021"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 10:00:57.078547: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.21GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6514 - loss: 0.6016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 10:00:58.864866: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.6515 - loss: 0.6015 - val_accuracy: 0.8002 - val_loss: 0.4517\n",
      "Epoch 2/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9472 - loss: 0.1567 - val_accuracy: 0.8348 - val_loss: 0.4072\n",
      "Epoch 3/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9904 - loss: 0.0424 - val_accuracy: 0.8397 - val_loss: 0.4584\n",
      "Epoch 4/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9945 - loss: 0.0234 - val_accuracy: 0.8385 - val_loss: 0.4959\n",
      "Epoch 5/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9959 - loss: 0.0207 - val_accuracy: 0.8220 - val_loss: 0.5994\n",
      "Epoch 6/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9959 - loss: 0.0170 - val_accuracy: 0.8195 - val_loss: 0.6551\n",
      "Epoch 7/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9963 - loss: 0.0124 - val_accuracy: 0.8201 - val_loss: 0.7197\n",
      "Epoch 8/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9962 - loss: 0.0130 - val_accuracy: 0.8138 - val_loss: 0.8290\n",
      "Epoch 9/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9979 - loss: 0.0070 - val_accuracy: 0.8164 - val_loss: 0.9202\n",
      "Epoch 10/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9968 - loss: 0.0100 - val_accuracy: 0.8200 - val_loss: 0.9136\n",
      "Epoch 11/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9976 - loss: 0.0080 - val_accuracy: 0.8038 - val_loss: 0.9861\n",
      "Epoch 12/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9978 - loss: 0.0063 - val_accuracy: 0.8127 - val_loss: 1.0219\n",
      "Epoch 13/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0060 - val_accuracy: 0.8146 - val_loss: 1.0548\n",
      "Epoch 14/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0043 - val_accuracy: 0.8163 - val_loss: 1.0884\n",
      "Epoch 15/15\n",
      "\u001b[1m1066/1066\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9975 - loss: 0.0077 - val_accuracy: 0.8234 - val_loss: 1.1430\n",
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8231 - loss: 1.0492\n",
      "Test Accuracy: 82.67%\n"
     ]
    }
   ],
   "source": [
    "CNNModel(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
